{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11756211,"sourceType":"datasetVersion","datasetId":7380339},{"sourceId":11756415,"sourceType":"datasetVersion","datasetId":7380442}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers datasets torch pillow bert_score tqdm nltk pandas matplotlib evaluate rouge_score py-rouge sacrebleu ftfy regex\n!pip install open-clip-torch  # Correct package name with hyphens\n\nimport os\nimport random\nimport torch\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport evaluate\nimport open_clip\nfrom PIL import Image\nfrom collections import Counter\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n# Import the rouge_scorer package for ROUGE-L calculation\ntry:\n    from rouge_score import rouge_scorer\nexcept ImportError:\n    # Install if not available\n    !pip install rouge-score\n    from rouge_score import rouge_scorer\n\n# Set random seed for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T06:09:34.081870Z","iopub.execute_input":"2025-05-11T06:09:34.082641Z","iopub.status.idle":"2025-05-11T06:11:26.696805Z","shell.execute_reply.started":"2025-05-11T06:09:34.082622Z","shell.execute_reply":"2025-05-11T06:11:26.696164Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nCollecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting py-rouge\n  Downloading py_rouge-1.1-py3-none-any.whl.metadata (8.7 kB)\nCollecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting ftfy\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading py_rouge-1.1-py3-none-any.whl (56 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=5bae087f2cb0cf4a6237b29747bc4aeb7458ad9b816ef2b612e8759396d89561\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: py-rouge, portalocker, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, ftfy, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sacrebleu, rouge_score, evaluate, bert_score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bert_score-0.3.13 evaluate-0.4.3 fsspec-2024.12.0 ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 portalocker-3.1.1 py-rouge-1.1 rouge_score-0.1.2 sacrebleu-2.5.1\nCollecting open-clip-torch\n  Downloading open_clip_torch-2.32.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2.5.1+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.20.1+cu124)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2024.11.6)\nRequirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (6.3.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (4.67.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.30.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.5.2)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (1.0.14)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.0->open-clip-torch) (1.3.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open-clip-torch) (0.2.13)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (2.32.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.0->open-clip-torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->open-clip-torch) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->open-clip-torch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->open-clip-torch) (2024.2.0)\nDownloading open_clip_torch-2.32.0-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: open-clip-torch\nSuccessfully installed open-clip-torch-2.32.0\n","output_type":"stream"},{"name":"stderr","text":"2025-05-11 06:11:08.022704: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746943868.241534      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746943868.305256      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c7a1275adf0>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Step 1: Verify directory structure\nprint(\"Input directory:\", os.listdir('/kaggle/input/'))\nprint(\"vqa-dataset contents:\", os.listdir('/kaggle/input/dataset'))\nprint(\"small folder contents:\", os.listdir('/kaggle/input/imagedataset/small'))\n\n# Step 2: Load the full dataset from CSV\ncsv_path = '/kaggle/input/dataset/Complete_vqa(in).csv'\nfull_dataset = load_dataset(\"csv\", data_files=csv_path)\n\n# Step 3: Update image paths to absolute paths\nbase_image_path = '/kaggle/input/imagedataset/small'\nfull_dataset = full_dataset.map(lambda example: {'image_path': os.path.join(base_image_path, example['image_path'])})\n\n# Step 4: Load the CLIP model\nprint(\"Loading CLIP model and processor...\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\nmodel = model.to(device)\n\n# Load BART model for BARTScore calculation\nprint(\"Loading BART model for BARTScore...\")\nfrom transformers import BartTokenizer, BartForConditionalGeneration\nbart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\nbart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\nbart_model.eval()\nbart_model = bart_model.to(device)\n\n# Initialize evaluation metrics\nbertscore = evaluate.load('bertscore')\n\n# Define answer candidates (common VQA answers)\ncommon_answers = [\n    \"yes\", \"no\", \"2\", \"1\", \"white\", \"red\", \"blue\", \"black\", \"3\", \"green\",\n    \"yellow\", \"4\", \"brown\", \"orange\", \"0\", \"5\", \"gray\", \"grey\", \"purple\", \"pink\",\n    \"6\", \"7\", \"8\", \"9\", \"10\", \"standing\", \"sitting\", \"right\", \"left\", \"wood\",\n    \"metal\", \"plastic\", \"glass\", \"paper\", \"water\", \"food\", \"man\", \"woman\", \"boy\",\n    \"girl\", \"dog\", \"cat\", \"table\", \"chair\", \"car\", \"phone\", \"book\", \"computer\",\n    \"tv\", \"bed\", \"outside\", \"inside\", \"day\", \"night\", \"morning\", \"evening\", \"afternoon\",\n    \"kitchen\", \"bathroom\", \"bedroom\", \"living room\", \"large\", \"small\", \"medium\",\n    \"round\", \"square\", \"rectangular\", \"triangle\", \"circle\", \"diamond\", \"heart\", \"star\",\n    \"up\", \"down\", \"on\", \"under\", \"near\", \"far\", \"tall\", \"short\", \"long\", \"wide\",\n    \"narrow\", \"big\", \"little\", \"high\", \"low\", \"hot\", \"cold\", \"warm\", \"cool\", \"old\",\n    \"new\", \"young\", \"open\", \"closed\"\n]\n\n# Step 5: Define preprocessing function\ndef preprocess_example(example):\n    image_path = example['image_path']\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n    \n    # Load and preprocess the image for CLIP\n    image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n    \n    # Prepare the question\n    question = example['question']\n    \n    return {'image': image, 'question': question}\n\n# Step 6: Define VQA prediction function using CLIP\ndef get_prediction(example):\n    try:\n        processed = preprocess_example(example)\n        image = processed['image']\n        question = processed['question']\n        \n        # Encode image\n        with torch.no_grad():\n            image_features = model.encode_image(image)\n            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n            \n            # Prepare text prompts (question + possible answer)\n            answer_prompts = [f\"Question: {question} Answer: {ans}\" for ans in common_answers]\n            \n            # Use open_clip's tokenizer\n            tokenizer = open_clip.get_tokenizer('ViT-B-32')\n            text = tokenizer(answer_prompts).to(device)\n            \n            # Encode text prompts\n            text_features = model.encode_text(text)\n            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n            \n            # Calculate similarity scores\n            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n            \n            # Get the most likely answer\n            values, indices = similarity[0].topk(1)\n            predicted_answer = common_answers[indices[0].item()]\n            \n        return predicted_answer\n    except Exception as e:\n        print(f\"Error in prediction: {e}\")\n        return \"\"\n\n# Step 7: Create sampling functions for different dataset sizes\ndef create_sampled_dataset(dataset, percentage):\n    dataset_size = len(dataset['train'])\n    sample_size = int(dataset_size * percentage)\n    \n    # Get random indices without replacement\n    indices = random.sample(range(dataset_size), sample_size)\n    \n    # Create sampled dataset\n    sampled_data = dataset['train'].select(indices)\n    return sampled_data\n\n# Step 8: Define evaluation metrics\ndef calculate_metrics(predictions, ground_truths):\n    metrics = {}\n    \n    # Ensure predictions and ground_truths are valid\n    valid_pairs = [(p, g) for p, g in zip(predictions, ground_truths) \n                    if isinstance(p, str) and isinstance(g, str)]\n    \n    if not valid_pairs:\n        return {\n            'accuracy': 0.0,\n            'f1_score': 0.0,\n            'bleu_score': 0.0,\n            'bert_score_p': 0.0,\n            'bert_score_r': 0.0,\n            'bert_score_f1': 0.0,\n            'bart_score': 0.0,\n            'rouge_l': 0.0\n        }\n    \n    valid_predictions, valid_ground_truths = zip(*valid_pairs)\n    \n    # Accuracy calculation\n    correct = sum(1 for p, g in zip(valid_predictions, valid_ground_truths) \n                 if p.lower() == g.lower())\n    metrics['accuracy'] = correct / len(valid_pairs) if valid_pairs else 0.0\n    \n    # F1 Score calculation (token level)\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    \n    for pred, truth in valid_pairs:\n        pred_tokens = set(pred.lower().split())\n        truth_tokens = set(truth.lower().split())\n        \n        true_positives += len(pred_tokens.intersection(truth_tokens))\n        false_positives += len(pred_tokens - truth_tokens)\n        false_negatives += len(truth_tokens - pred_tokens)\n    \n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n    metrics['f1_score'] = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    # BLEU Score\n    smoothie = SmoothingFunction().method1\n    bleu_scores = []\n    \n    for pred, truth in valid_pairs:\n        pred_tokens = nltk.word_tokenize(pred.lower())\n        truth_tokens = [nltk.word_tokenize(truth.lower())]\n        try:\n            bleu = sentence_bleu(truth_tokens, pred_tokens, smoothing_function=smoothie)\n            bleu_scores.append(bleu)\n        except Exception as e:\n            print(f\"BLEU calculation error: {e}\")\n            bleu_scores.append(0)\n    \n    metrics['bleu_score'] = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n    \n    # BERTScore using evaluate library\n    try:\n        bert_results = bertscore.compute(\n            predictions=valid_predictions, \n            references=valid_ground_truths, \n            lang=\"en\",\n            model_type=\"microsoft/deberta-xlarge-mnli\"\n        )\n        metrics['bert_score_p'] = sum(bert_results['precision']) / len(bert_results['precision'])\n        metrics['bert_score_r'] = sum(bert_results['recall']) / len(bert_results['recall'])\n        metrics['bert_score_f1'] = sum(bert_results['f1']) / len(bert_results['f1'])\n    except Exception as e:\n        print(f\"BERTScore calculation error with evaluate library: {e}\")\n        # Fallback to bert_score package\n        try:\n            # Using smaller batches to avoid memory issues\n            batch_size = 32\n            all_bert_p = []\n            all_bert_r = []\n            all_bert_f1 = []\n            \n            for i in range(0, len(valid_predictions), batch_size):\n                batch_preds = valid_predictions[i:i+batch_size]\n                batch_refs = valid_ground_truths[i:i+batch_size]\n                \n                P, R, F1 = bert_score_calc(batch_preds, batch_refs, lang=\"en\", verbose=False)\n                all_bert_p.extend(P.tolist())\n                all_bert_r.extend(R.tolist())\n                all_bert_f1.extend(F1.tolist())\n            \n            metrics['bert_score_p'] = sum(all_bert_p) / len(all_bert_p) if all_bert_p else 0\n            metrics['bert_score_r'] = sum(all_bert_r) / len(all_bert_r) if all_bert_r else 0\n            metrics['bert_score_f1'] = sum(all_bert_f1) / len(all_bert_f1) if all_bert_f1 else 0\n        except Exception as e:\n            print(f\"BERTScore fallback calculation error: {e}\")\n            metrics['bert_score_p'] = 0\n            metrics['bert_score_r'] = 0\n            metrics['bert_score_f1'] = 0\n    \n    # BARTScore calculation\n    bart_scores = []\n    try:\n        with torch.no_grad():\n            for pred, ref in tqdm(list(zip(valid_predictions, valid_ground_truths)), desc=\"Calculating BARTScore\", leave=False):\n                # Skip empty strings\n                if not pred.strip() or not ref.strip():\n                    continue\n                \n                # Encode the prediction and reference\n                inputs = bart_tokenizer(ref, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n                with torch.no_grad():\n                    pred_encoded = bart_tokenizer(pred, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n                    ref_ids = inputs[\"input_ids\"]\n                    \n                    # Calculate log likelihood\n                    outputs = bart_model(\n                        input_ids=pred_encoded[\"input_ids\"],\n                        attention_mask=pred_encoded[\"attention_mask\"],\n                        labels=ref_ids\n                    )\n                    neg_log_likelihood = outputs.loss.item()\n                    # Convert negative log likelihood to score (higher is better)\n                    bart_score = -neg_log_likelihood\n                    bart_scores.append(bart_score)\n        \n        metrics['bart_score'] = sum(bart_scores) / len(bart_scores) if bart_scores else 0\n    except Exception as e:\n        print(f\"BARTScore calculation error: {e}\")\n        metrics['bart_score'] = 0\n    \n    # ROUGE-L Score\n    try:\n        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n        rouge_scores = []\n        \n        for pred, truth in valid_pairs:\n            score = scorer.score(truth, pred)\n            rouge_scores.append(score['rougeL'].fmeasure)\n        \n        metrics['rouge_l'] = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0\n    except Exception as e:\n        print(f\"ROUGE-L calculation error: {e}\")\n        metrics['rouge_l'] = 0\n    \n    return metrics\n\n# Step 9: Define evaluation function with progress bar\ndef evaluate_dataset(dataset_sample, name):\n    print(f\"\\nEvaluating {name} dataset ({len(dataset_sample)} samples)...\")\n    predictions = []\n    ground_truths = []\n    \n    # Use tqdm for progress bar\n    for example in tqdm(dataset_sample, desc=f\"Processing {name} dataset\"):\n        try:\n            predicted_answer = get_prediction(example)\n            ground_truth = example['answer']\n            \n            # Skip if ground_truth or predicted_answer is invalid\n            if (ground_truth is None or not isinstance(ground_truth, str) or\n                predicted_answer is None or not isinstance(predicted_answer, str)):\n                continue\n            \n            predictions.append(predicted_answer)\n            ground_truths.append(ground_truth)\n        \n        except FileNotFoundError as e:\n            print(f\"FileNotFoundError: {e}\")\n            continue\n        except Exception as e:\n            print(f\"Error processing example: {e}\")\n            continue\n    \n    # Calculate metrics\n    metrics = calculate_metrics(predictions, ground_truths)\n    \n    # Print metrics\n    print(f\"\\n{name} Evaluation Results:\")\n    print(f\"Number of samples evaluated: {len(predictions)}\")\n    print(f\"Accuracy: {metrics['accuracy'] * 100:.2f}%\")\n    print(f\"F1 Score: {metrics['f1_score'] * 100:.2f}%\")\n    print(f\"BLEU Score: {metrics['bleu_score'] * 100:.2f}%\")\n    print(f\"BERTScore Precision: {metrics['bert_score_p'] * 100:.2f}%\")\n    print(f\"BERTScore Recall: {metrics['bert_score_r'] * 100:.2f}%\")\n    print(f\"BERTScore F1: {metrics['bert_score_f1'] * 100:.2f}%\")\n    print(f\"BARTScore: {metrics['bart_score']:.4f}\")\n    print(f\"ROUGE-L: {metrics['rouge_l'] * 100:.2f}%\")\n    \n    return metrics\n\n# Step 10: Create different sized datasets\nprint(f\"\\nTotal dataset size: {len(full_dataset['train'])} samples\")\n\n# Create different sized datasets\nfull_sample = full_dataset['train']\nhalf_sample = create_sampled_dataset(full_dataset, 0.5)\nquarter_sample = create_sampled_dataset(full_dataset, 0.25)\n\nprint(f\"Full dataset: {len(full_sample)} samples\")\nprint(f\"50% dataset: {len(half_sample)} samples\")\nprint(f\"25% dataset: {len(quarter_sample)} samples\")\n\n# Step 11: Evaluate and collect results\nresults = {}\n\n# Evaluate quarter dataset\nresults['25% Dataset'] = evaluate_dataset(quarter_sample, \"25% Dataset\")\n\n# Evaluate half dataset\nresults['50% Dataset'] = evaluate_dataset(half_sample, \"50% Dataset\")\n\n# Evaluate full dataset\nresults['Full Dataset'] = evaluate_dataset(full_sample, \"Full Dataset\")\n\n# Step 12: Create comparison charts\ndef plot_metrics_comparison(results):\n    metrics_to_plot = ['accuracy', 'f1_score', 'bleu_score', 'bert_score_f1', 'bart_score', 'rouge_l']\n    display_names = {\n        'accuracy': 'Accuracy',\n        'f1_score': 'F1 Score',\n        'bleu_score': 'BLEU Score',\n        'bert_score_f1': 'BERTScore F1',\n        'bart_score': 'BARTScore',\n        'rouge_l': 'ROUGE-L'\n    }\n    \n    # Create figure with subplots - 2 rows of 3 metrics each\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    \n    # Plot each metric\n    axes = axes.flatten()\n    \n    for i, metric in enumerate(metrics_to_plot):\n        dataset_names = list(results.keys())\n        \n        # Handle BARTScore differently as it's not a percentage\n        if metric == 'bart_score':\n            values = [results[dataset][metric] for dataset in dataset_names]\n            bars = axes[i].bar(dataset_names, values)\n            axes[i].set_title(display_names[metric])\n            axes[i].set_ylabel('Score')\n            \n            # Add value labels on bars\n            for bar in bars:\n                height = bar.get_height()\n                axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                             f'{height:.4f}', ha='center', va='bottom')\n        else:\n            values = [results[dataset][metric] * 100 for dataset in dataset_names]\n            bars = axes[i].bar(dataset_names, values)\n            axes[i].set_title(display_names[metric])\n            axes[i].set_ylabel('Score (%)')\n            axes[i].set_ylim(0, 100)\n            \n            # Add value labels on bars\n            for bar in bars:\n                height = bar.get_height()\n                axes[i].text(bar.get_x() + bar.get_width()/2., height + 1,\n                             f'{height:.2f}%', ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.savefig('clip_metrics_comparison.png')\n    plt.close()\n    \n    # Create dataset size comparison\n    dataset_sizes = {\n        '25% Dataset': len(quarter_sample),\n        '50% Dataset': len(half_sample),\n        'Full Dataset': len(full_sample)\n    }\n    \n    plt.figure(figsize=(10, 6))\n    bars = plt.bar(dataset_sizes.keys(), dataset_sizes.values())\n    plt.title('Dataset Size Comparison')\n    plt.ylabel('Number of Samples')\n    \n    # Add value labels on bars\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n                 f'{height}', ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.savefig('dataset_size_comparison.png')\n    plt.close()\n\n# Generate comparison plots\nplot_metrics_comparison(results)\n\n# Save results to CSV\nresults_df = pd.DataFrame({\n    'Dataset': list(results.keys()),\n    'Size': [len(quarter_sample), len(half_sample), len(full_sample)],\n    'Accuracy (%)': [results[ds]['accuracy'] * 100 for ds in results],\n    'F1 Score (%)': [results[ds]['f1_score'] * 100 for ds in results],\n    'BLEU Score (%)': [results[ds]['bleu_score'] * 100 for ds in results],\n    'BERTScore P (%)': [results[ds]['bert_score_p'] * 100 for ds in results],\n    'BERTScore R (%)': [results[ds]['bert_score_r'] * 100 for ds in results],\n    'BERTScore F1 (%)': [results[ds]['bert_score_f1'] * 100 for ds in results],\n    'BARTScore': [results[ds]['bart_score'] for ds in results],\n    'ROUGE-L (%)': [results[ds]['rouge_l'] * 100 for ds in results]\n})\n\nresults_df.to_csv('clip_evaluation_results.csv', index=False)\nprint(\"\\nResults saved to clip_evaluation_results.csv\")\nprint(\"\\nEvaluation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T09:38:19.278681Z","iopub.execute_input":"2025-05-10T09:38:19.279010Z","execution_failed":"2025-05-10T11:28:08.696Z"}},"outputs":[{"name":"stdout","text":"Input directory: ['imagedataset', 'dataset']\nvqa-dataset contents: ['Complete_vqa(in).csv']\nsmall folder contents: ['d8', '0d', '47', '17', '81', 'c5', '19', 'e2', '8f', 'a2', '22', 'cf', 'b8', '35', '92', 'b2', '50', 'c7', '23', '5b', '87', '07', 'd5', '10', 'd0', 'cd', '61', '1b', 'bf', '2e', '36', '05', '20', '06', '0f', '45', '60', '27', '6d', '64', '41', 'c8', '9e', 'da', 'ff', '89', '39', '0c', '4d', '02', '32', '98', '25', '42', 'b1', 'e5', 'b4', '52', 'f0', 'ac', 'd9', 'e0', '75', '0a', '8b', '2c', '38', '7d', '12', 'dc', 'ef', '94', '55', 'a0', 'e1', 'e7', '04', 'be', 'f1', '7e', 'ea', '49', 'fc', 'e3', 'e4', 'b9', '6e', '31', 'f9', '62', 'b6', '53', '1a', 'd2', '70', '34', '18', '4b', 'db', '79', '85', 'c2', '88', '65', '1e', '67', 'ec', 'ab', 'a3', 'a7', '78', '0e', 'd6', '28', 'f4', '66', 'cb', 'a5', 'images.csv', 'a8', 'bc', 'e6', '56', '72', '6c', '16', '7b', 'bb', 'af', '9f', 'bd', 'ba', '13', '99', '1d', '4e', '3e', '7f', '26', 'b5', 'fb', 'd4', '4c', '74', 'b0', 'c6', '00', 'c4', '08', '15', 'ae', '90', '69', 'balanced_dataset.csv', 'f8', '77', 'd1', 'f5', '3f', '9b', 'cc', '86', '95', 'f3', '43', 'ce', 'eb', '91', '3c', '71', 'ca', 'ed', '09', '58', '59', '5e', 'fe', '4f', '7c', 'c3', '2d', '2b', 'a4', '97', '30', '7a', '14', '6f', 'f6', '5a', '03', '76', '3a', 'a6', '8e', 'a9', '8d', '9d', '84', 'df', '3d', '1f', 'b7', '83', 'f2', '82', '57', 'e8', 'd7', 'complete_dataset.csv', 'dd', 'aa', '5f', '1c', '96', '46', '6b', 'a1', '21', '44', '40', '80', '9a', '11', 'de', '68', '4a', 'vqa.csv', '63', 'ad', 'd3', '37', '51', 'f7', 'e9', '01', '8c', '9c', 'fa', '33', 'c1', 'b3', '54', '48', '0b', '5c', 'ee', '8a', 'c9', 'c0', '29', '3b', 'fd', '5d', '2f', '24', '6a', '73', '93', '2a']\nLoading CLIP model and processor...\nLoading BART model for BARTScore...\n\nTotal dataset size: 45482 samples\nFull dataset: 45482 samples\n50% dataset: 22741 samples\n25% dataset: 11370 samples\n\nEvaluating 25% Dataset dataset (11370 samples)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing 25% Dataset dataset:   0%|          | 0/11370 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df40ecd2ac9543ffbc344f789445bb1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7032f43b49604cc4a772648f416df1c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf1b912f2287401c8a4e97ebb2ae2d29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"691c6b4d3b9a4e0e925614b5a24ccba1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f8d7d30558a4bc4bd5e679c10722d9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.04G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53c7764aad0f4b868489cd9af75b3089"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.04G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2333e42634f14020b28090be460160ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Calculating BARTScore:   0%|          | 0/11360 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n25% Dataset Evaluation Results:\nNumber of samples evaluated: 11360\nAccuracy: 9.92%\nF1 Score: 9.89%\nBLEU Score: 1.76%\nBERTScore Precision: 58.12%\nBERTScore Recall: 57.61%\nBERTScore F1: 57.72%\nBARTScore: -6.7001\nROUGE-L: 10.04%\n\nEvaluating 50% Dataset dataset (22741 samples)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing 50% Dataset dataset:   0%|          | 0/22741 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c906254272d43a08636543a6b71807d"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Step 1: Verify directory structure\nprint(\"Input directory:\", os.listdir('/kaggle/input/'))\nprint(\"vqa-dataset contents:\", os.listdir('/kaggle/input/dataset'))\nprint(\"small folder contents:\", os.listdir('/kaggle/input/imagedataset/small'))\n\n# Step 2: Load the full dataset from CSV\ncsv_path = '/kaggle/input/dataset/Complete_vqa(in).csv'\nfull_dataset = load_dataset(\"csv\", data_files=csv_path)\n\n# Step 3: Update image paths to absolute paths\nbase_image_path = '/kaggle/input/imagedataset/small'\nfull_dataset = full_dataset.map(lambda example: {'image_path': os.path.join(base_image_path, example['image_path'])})\n\n# Step 4: Load the CLIP model\nprint(\"Loading CLIP model and processor...\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\nmodel = model.to(device)\n\n# Load BART model for BARTScore calculation\nprint(\"Loading BART model for BARTScore...\")\nbart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\nbart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\nbart_model.eval()\nbart_model = bart_model.to(device)\n\n# Initialize evaluation metrics\nbertscore = evaluate.load('bertscore')\n\n# Step 5: Extract all unique answers from the dataset (optimized for one-word answers)\ndef extract_unique_answers(dataset):\n    # Get all answers and convert to lowercase for consistency\n    all_answers = [example['answer'].lower().strip() for example in dataset \n                  if isinstance(example['answer'], str) and example['answer'].strip()]\n    \n    # Count answer frequencies\n    answer_counts = Counter(all_answers)\n    \n    # For one-word answers, we can be more inclusive with frequency filtering\n    # but still filter very rare answers to avoid overfitting\n    min_count = 5  # Adjust based on dataset size\n    filtered_answers = [ans for ans, count in answer_counts.items() if count >= min_count]\n    \n    # Explicitly filter to ensure we only have single-word answers\n    # This helps with matching efficiency and removes potential multi-word outliers\n    single_word_answers = [ans for ans in filtered_answers if len(ans.split()) == 1]\n    \n    # Sort by frequency (prioritizing common answers)\n    single_word_answers.sort(key=lambda x: answer_counts[x], reverse=True)\n    \n    print(f\"Extracted {len(single_word_answers)} unique single-word answers from dataset\")\n    print(f\"Original dataset had {len(answer_counts)} distinct answers\")\n    print(f\"After frequency filtering ({min_count}+ occurrences): {len(filtered_answers)} answers\")\n    print(f\"After ensuring single-word only: {len(single_word_answers)} answers\")\n    \n    return single_word_answers\n\n# Extract unique answers\nunique_answers = extract_unique_answers(full_dataset['train'])\n\n# Step 6: Define preprocessing function\ndef preprocess_example(example):\n    image_path = example['image_path']\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n    \n    # Load and preprocess the image for CLIP\n    image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n    \n    # Prepare the question\n    question = example['question']\n    \n    return {'image': image, 'question': question}\n\n# Step 7: Define VQA prediction function using CLIP (optimized for single-word answers)\ndef get_prediction(example, unique_answers, batch_size=512):\n    try:\n        processed = preprocess_example(example)\n        image = processed['image']\n        question = processed['question']\n        \n        # Encode image once\n        with torch.no_grad():\n            image_features = model.encode_image(image)\n            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n            \n            # For single-word answers, we can use larger batch sizes safely\n            # We can also use more specific prompt templates for better accuracy\n            best_score = -float('inf')\n            best_answer = \"\"\n            \n            for i in range(0, len(unique_answers), batch_size):\n                batch_answers = unique_answers[i:i+batch_size]\n                \n                # Create more specific prompts for single-word answers\n                # This template helps CLIP distinguish between similar concepts better\n                answer_prompts = [f\"The answer to the question '{question}' is {ans}.\" for ans in batch_answers]\n                \n                # Use open_clip's tokenizer\n                tokenizer = open_clip.get_tokenizer('ViT-B-32')\n                text = tokenizer(answer_prompts).to(device)\n                \n                # Encode text prompts\n                text_features = model.encode_text(text)\n                text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n                \n                # Calculate similarity scores\n                similarity = (100.0 * image_features @ text_features.T)\n                \n                # Update best answer if found\n                batch_best_score, batch_best_idx = similarity[0].max(dim=0)\n                if batch_best_score > best_score:\n                    best_score = batch_best_score\n                    best_answer = batch_answers[batch_best_idx.item()]\n            \n        return best_answer\n    except Exception as e:\n        print(f\"Error in prediction: {e}\")\n        return \"\"\n\n# Step 8: Create sampling functions for different dataset sizes\ndef create_sampled_dataset(dataset, percentage):\n    dataset_size = len(dataset['train'])\n    sample_size = int(dataset_size * percentage)\n    \n    # Get random indices without replacement\n    indices = random.sample(range(dataset_size), sample_size)\n    \n    # Create sampled dataset\n    sampled_data = dataset['train'].select(indices)\n    return sampled_data\n\n# Import necessary NLTK components\ntry:\n    from nltk.metrics.distance import edit_distance\nexcept ImportError:\n    nltk.download('punkt')\n    from nltk.metrics.distance import edit_distance\n\n# Step 9: Define evaluation metrics (optimized for single-word answers)\ndef calculate_metrics(predictions, ground_truths):\n    metrics = {}\n    \n    # Ensure predictions and ground_truths are valid\n    valid_pairs = [(p, g) for p, g in zip(predictions, ground_truths) \n                    if isinstance(p, str) and isinstance(g, str)]\n    \n    if not valid_pairs:\n        return {\n            'accuracy': 0.0,\n            'f1_score': 0.0,\n            'bleu_score': 0.0,\n            'bert_score_p': 0.0,\n            'bert_score_r': 0.0,\n            'bert_score_f1': 0.0,\n            'bart_score': 0.0,\n            'rouge_l': 0.0\n        }\n    \n    valid_predictions, valid_ground_truths = zip(*valid_pairs)\n    \n    # For single-word answers, exact match accuracy is the most important metric\n    # Normalize everything to lowercase single words for fair comparison\n    norm_predictions = [p.lower().strip() for p in valid_predictions]\n    norm_ground_truths = [g.lower().strip() for g in valid_ground_truths]\n    \n    # Exact match accuracy\n    correct = sum(1 for p, g in zip(norm_predictions, norm_ground_truths) if p == g)\n    metrics['accuracy'] = correct / len(valid_pairs) if valid_pairs else 0.0\n    \n    # Also calculate \"close match\" accuracy (for words that might have slight spelling variations)\n    # This is particularly useful for single-word answers\n    close_matches = 0\n    for p, g in zip(norm_predictions, norm_ground_truths):\n        # If exact match or Levenshtein distance <= 1 (handles minor typos)\n        if p == g or (len(p) > 2 and len(g) > 2 and nltk.edit_distance(p, g) <= 1):\n            close_matches += 1\n    \n    metrics['close_match_accuracy'] = close_matches / len(valid_pairs) if valid_pairs else 0.0\n    \n    # F1 Score calculation (token level)\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    \n    for pred, truth in valid_pairs:\n        pred_tokens = set(pred.lower().split())\n        truth_tokens = set(truth.lower().split())\n        \n        true_positives += len(pred_tokens.intersection(truth_tokens))\n        false_positives += len(pred_tokens - truth_tokens)\n        false_negatives += len(truth_tokens - pred_tokens)\n    \n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n    metrics['f1_score'] = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    # BLEU Score\n    smoothie = SmoothingFunction().method1\n    bleu_scores = []\n    \n    for pred, truth in valid_pairs:\n        pred_tokens = nltk.word_tokenize(pred.lower())\n        truth_tokens = [nltk.word_tokenize(truth.lower())]\n        try:\n            bleu = sentence_bleu(truth_tokens, pred_tokens, smoothing_function=smoothie)\n            bleu_scores.append(bleu)\n        except Exception as e:\n            print(f\"BLEU calculation error: {e}\")\n            bleu_scores.append(0)\n    \n    metrics['bleu_score'] = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n    \n    # BERTScore using evaluate library\n    try:\n        bert_results = bertscore.compute(\n            predictions=valid_predictions, \n            references=valid_ground_truths, \n            lang=\"en\",\n            model_type=\"microsoft/deberta-xlarge-mnli\"\n        )\n        metrics['bert_score_p'] = sum(bert_results['precision']) / len(bert_results['precision'])\n        metrics['bert_score_r'] = sum(bert_results['recall']) / len(bert_results['recall'])\n        metrics['bert_score_f1'] = sum(bert_results['f1']) / len(bert_results['f1'])\n    except Exception as e:\n        print(f\"BERTScore calculation error: {e}\")\n        metrics['bert_score_p'] = 0\n        metrics['bert_score_r'] = 0\n        metrics['bert_score_f1'] = 0\n    \n    # BARTScore calculation\n    bart_scores = []\n    try:\n        with torch.no_grad():\n            for pred, ref in zip(valid_predictions, valid_ground_truths):\n                # Skip empty strings\n                if not pred.strip() or not ref.strip():\n                    continue\n                \n                # Encode the prediction and reference\n                inputs = bart_tokenizer(ref, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n                with torch.no_grad():\n                    pred_encoded = bart_tokenizer(pred, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n                    ref_ids = inputs[\"input_ids\"]\n                    \n                    # Calculate log likelihood\n                    outputs = bart_model(\n                        input_ids=pred_encoded[\"input_ids\"],\n                        attention_mask=pred_encoded[\"attention_mask\"],\n                        labels=ref_ids\n                    )\n                    neg_log_likelihood = outputs.loss.item()\n                    # Convert negative log likelihood to score (higher is better)\n                    bart_score = -neg_log_likelihood\n                    bart_scores.append(bart_score)\n        \n        metrics['bart_score'] = sum(bart_scores) / len(bart_scores) if bart_scores else 0\n    except Exception as e:\n        print(f\"BARTScore calculation error: {e}\")\n        metrics['bart_score'] = 0\n    \n    # ROUGE-L Score calculation\n    try:\n        # Initialize rouge scorer with rouge-l\n        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n        \n        rouge_l_scores = []\n        for pred, ref in zip(valid_predictions, valid_ground_truths):\n            # Calculate ROUGE-L score\n            scores = scorer.score(ref, pred)\n            rouge_l_scores.append(scores['rougeL'].fmeasure)\n        \n        metrics['rouge_l'] = sum(rouge_l_scores) / len(rouge_l_scores) if rouge_l_scores else 0\n    except Exception as e:\n        print(f\"ROUGE-L calculation error: {e}\")\n        metrics['rouge_l'] = 0\n    \n    return metrics\n\n# Step 10: Define evaluation function with progress bar (optimized for single-word answers)\ndef evaluate_dataset(dataset_sample, name):\n    print(f\"\\nEvaluating {name} dataset ({len(dataset_sample)} samples)...\")\n    predictions = []\n    ground_truths = []\n    \n    # Track error cases to help with debugging\n    missing_files = 0\n    other_errors = 0\n    \n    # Also track error cases by type\n    error_types = Counter()\n    \n    # Use tqdm for progress bar\n    for example in tqdm(dataset_sample, desc=f\"Processing {name} dataset\"):\n        try:\n            # Use the optimized prediction function with unique answers from dataset\n            predicted_answer = get_prediction(example, unique_answers)\n            ground_truth = example['answer']\n            \n            # Skip if ground_truth or predicted_answer is invalid\n            if (ground_truth is None or not isinstance(ground_truth, str) or\n                predicted_answer is None or not isinstance(predicted_answer, str)):\n                continue\n            \n            # For single-word answers, normalize by trimming and lowercasing\n            predictions.append(predicted_answer.lower().strip())\n            ground_truths.append(ground_truth.lower().strip())\n        \n        except FileNotFoundError as e:\n            missing_files += 1\n            if missing_files < 10:  # Only print first few errors to avoid spam\n                print(f\"FileNotFoundError: {e}\")\n            continue\n        except Exception as e:\n            other_errors += 1\n            error_type = type(e).__name__\n            error_types[error_type] += 1\n            if other_errors < 10:  # Only print first few errors to avoid spam\n                print(f\"Error processing example: {e}\")\n            continue\n    \n    # Report error statistics\n    if missing_files > 0:\n        print(f\"Missing files: {missing_files}\")\n    if other_errors > 0:\n        print(f\"Other errors: {other_errors}\")\n        print(\"Error types distribution:\", dict(error_types))\n    \n    # Calculate metrics\n    metrics = calculate_metrics(predictions, ground_truths)\n    \n    # Print metrics with focus on accuracy for single-word answers\n    print(f\"\\n{name} Evaluation Results:\")\n    print(f\"Number of samples evaluated: {len(predictions)}\")\n    print(f\"Exact Match Accuracy: {metrics['accuracy'] * 100:.2f}%\")\n    print(f\"Close Match Accuracy (with typo tolerance): {metrics['close_match_accuracy'] * 100:.2f}%\")\n    \n    # Print other metrics\n    if 'f1_score' in metrics:\n        print(f\"F1 Score: {metrics['f1_score'] * 100:.2f}%\")\n    if 'bleu_score' in metrics:\n        print(f\"BLEU Score: {metrics['bleu_score'] * 100:.2f}%\")\n    if 'bert_score_f1' in metrics:\n        print(f\"BERTScore F1: {metrics['bert_score_f1'] * 100:.2f}%\")\n    if 'rouge_l' in metrics:\n        print(f\"ROUGE-L Score: {metrics['rouge_l'] * 100:.2f}%\")\n    \n    # For single-word answers, create confusion matrix for most common errors\n    # This is extremely helpful for debugging and improving performance\n    errors = [(pred, truth) for pred, truth in zip(predictions, ground_truths) if pred != truth]\n    if errors:\n        print(\"\\nTop misclassifications:\")\n        error_pairs = Counter(errors)\n        for (pred, truth), count in error_pairs.most_common(10):\n            print(f\"Predicted '{pred}' instead of '{truth}': {count} times\")\n    \n    return metrics\n\n# Step 11: Create different sized datasets\nprint(f\"\\nTotal dataset size: {len(full_dataset['train'])} samples\")\n\n# Create different sized datasets\nfull_sample = full_dataset['train']\nhalf_sample = create_sampled_dataset(full_dataset, 0.5)\nquarter_sample = create_sampled_dataset(full_dataset, 0.25)\n\nprint(f\"Full dataset: {len(full_sample)} samples\")\nprint(f\"50% dataset: {len(half_sample)} samples\")\nprint(f\"25% dataset: {len(quarter_sample)} samples\")\n\n# Step 12: Evaluate and collect results\nresults = {}\n\n# Evaluate quarter dataset\nresults['25% Dataset'] = evaluate_dataset(quarter_sample, \"25% Dataset\")\n\n# Evaluate half dataset\nresults['50% Dataset'] = evaluate_dataset(half_sample, \"50% Dataset\")\n\n# Evaluate full dataset\nresults['Full Dataset'] = evaluate_dataset(full_sample, \"Full Dataset\")\n\n# Step 13: Create comparison charts (optimized for single-word answers)\ndef plot_metrics_comparison(results):\n    # For single-word answers, focus on accuracy metrics\n    metrics_to_plot = ['accuracy', 'close_match_accuracy']\n    if 'f1_score' in results['Full Dataset']:\n        metrics_to_plot.extend(['f1_score', 'bleu_score', 'bert_score_f1', 'rouge_l'])\n    \n    display_names = {\n        'accuracy': 'Exact Match Accuracy',\n        'close_match_accuracy': 'Close Match Accuracy',\n        'f1_score': 'F1 Score',\n        'bleu_score': 'BLEU Score',\n        'bert_score_f1': 'BERTScore F1',\n        'rouge_l': 'ROUGE-L'\n    }\n    \n    # Create figure with subplots - 2 rows of 3 metrics each\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    \n    # Plot each metric\n    axes = axes.flatten()\n    \n    for i, metric in enumerate(metrics_to_plot):\n        dataset_names = list(results.keys())\n        \n        # Handle BARTScore differently as it's not a percentage\n        if metric == 'bart_score':\n            values = [results[dataset][metric] for dataset in dataset_names]\n            bars = axes[i].bar(dataset_names, values)\n            axes[i].set_title(display_names[metric])\n            axes[i].set_ylabel('Score')\n            \n            # Add value labels on bars\n            for bar in bars:\n                height = bar.get_height()\n                axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                             f'{height:.4f}', ha='center', va='bottom')\n        else:\n            values = [results[dataset][metric] * 100 for dataset in dataset_names]\n            bars = axes[i].bar(dataset_names, values)\n            axes[i].set_title(display_names[metric])\n            axes[i].set_ylabel('Score (%)')\n            axes[i].set_ylim(0, 100)\n            \n            # Add value labels on bars\n            for bar in bars:\n                height = bar.get_height()\n                axes[i].text(bar.get_x() + bar.get_width()/2., height + 1,\n                             f'{height:.2f}%', ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.savefig('clip_metrics_comparison.png')\n    plt.close()\n    \n    # Create dataset size comparison\n    dataset_sizes = {\n        '25% Dataset': len(quarter_sample),\n        '50% Dataset': len(half_sample),\n        'Full Dataset': len(full_sample)\n    }\n    \n    plt.figure(figsize=(10, 6))\n    bars = plt.bar(dataset_sizes.keys(), dataset_sizes.values())\n    plt.title('Dataset Size Comparison')\n    plt.ylabel('Number of Samples')\n    \n    # Add value labels on bars\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n                 f'{height}', ha='center', va='bottom')\n    \n    plt.tight_layout()\n    plt.savefig('dataset_size_comparison.png')\n    plt.close()\n\n# Step 14: Save results to CSV (optimized for single-word answers)\nresults_dict = {\n    'Dataset': list(results.keys()),\n    'Size': [len(quarter_sample), len(half_sample), len(full_sample)],\n    'Exact Match Accuracy (%)': [results[ds]['accuracy'] * 100 for ds in results],\n    'Close Match Accuracy (%)': [results[ds]['close_match_accuracy'] * 100 for ds in results],\n}\n\n# Add other metrics if they exist\nif 'f1_score' in results['Full Dataset']:\n    results_dict.update({\n        'F1 Score (%)': [results[ds]['f1_score'] * 100 for ds in results if 'f1_score' in results[ds]],\n        'BLEU Score (%)': [results[ds]['bleu_score'] * 100 for ds in results if 'bleu_score' in results[ds]],\n        'BERTScore F1 (%)': [results[ds]['bert_score_f1'] * 100 for ds in results if 'bert_score_f1' in results[ds]],\n        'ROUGE-L (%)': [results[ds]['rouge_l'] * 100 for ds in results if 'rouge_l' in results[ds]]\n    })\n\nresults_df = pd.DataFrame(results_dict)\n\n# Add information about unique answers and approach\nprint(f\"\\nUsing {len(unique_answers)} unique single-word answers from the dataset\")\n\nresults_df.to_csv('clip_evaluation_results.csv', index=False)\nprint(\"\\nResults saved to clip_evaluation_results.csv\")\nprint(\"\\nEvaluation complete!\")\n\n# Additional analysis for single-word answers\nprint(\"\\nAnalysis of single-word answers in the dataset:\")\nall_answers = [example['answer'].lower().strip() for example in full_dataset['train'] \n              if isinstance(example['answer'], str) and example['answer'].strip()]\nanswer_counts = Counter(all_answers)\n\n# Print the top answers with their frequencies\nprint(\"\\nTop 30 most common answers in the dataset:\")\nfor answer, count in answer_counts.most_common(30):\n    print(f\"{answer}: {count} occurrences\")\n\n# Print distribution statistics\ntotal_answers = len(all_answers)\nunique_count = len(answer_counts)\ntop_20_count = sum(count for _, count in answer_counts.most_common(20))\ntop_100_count = sum(count for _, count in answer_counts.most_common(100))\n\nprint(f\"\\nTotal answers in dataset: {total_answers}\")\nprint(f\"Unique answers: {unique_count}\")\nprint(f\"Top 20 answers cover {top_20_count/total_answers*100:.2f}% of all examples\")\nprint(f\"Top 100 answers cover {top_100_count/total_answers*100:.2f}% of all examples\")\n\n# Create a histogram of answer frequencies\nplt.figure(figsize=(12, 6))\nfrequency_counts = Counter([count for _, count in answer_counts.items()])\nitems = sorted(frequency_counts.items())\nx, y = zip(*items)\nplt.bar(range(len(x)), y, align='center')\nplt.xticks(range(len(x)), x, rotation=90)\nplt.xlabel('Answer Frequency')\nplt.ylabel('Number of Unique Answers')\nplt.title('Distribution of Answer Frequencies')\nplt.tight_layout()\nplt.savefig('answer_frequency_distribution.png')\nplt.close()\n\nprint(\"\\nSaved answer frequency distribution to 'answer_frequency_distribution.png'\")\n\n# Analyze answer characteristics for single words\nword_lengths = [len(answer) for answer in answer_counts.keys()]\navg_length = sum(word_lengths) / len(word_lengths)\n\nprint(f\"\\nAverage answer length: {avg_length:.2f} characters\")\nprint(f\"Shortest answer: {min(answer_counts.keys(), key=len)} ({len(min(answer_counts.keys(), key=len))} chars)\")\nprint(f\"Longest answer: {max(answer_counts.keys(), key=len)} ({len(max(answer_counts.keys(), key=len))} chars)\")\n\n# Save list of all unique answers for reference\nwith open('unique_answers.txt', 'w') as f:\n    for answer in unique_answers:\n        f.write(f\"{answer}\\n\")\n\nprint(\"\\nSaved list of unique answers used to 'unique_answers.txt'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T21:46:28.665218Z","iopub.execute_input":"2025-05-10T21:46:28.665520Z","iopub.status.idle":"2025-05-11T03:39:21.334464Z","shell.execute_reply.started":"2025-05-10T21:46:28.665494Z","shell.execute_reply":"2025-05-11T03:39:21.333339Z"}},"outputs":[{"name":"stdout","text":"Input directory: ['imagedataset', 'dataset']\nvqa-dataset contents: ['Complete_vqa(in).csv']\nsmall folder contents: ['d8', '0d', '47', '17', '81', 'c5', '19', 'e2', '8f', 'a2', '22', 'cf', 'b8', '35', '92', 'b2', '50', 'c7', '23', '5b', '87', '07', 'd5', '10', 'd0', 'cd', '61', '1b', 'bf', '2e', '36', '05', '20', '06', '0f', '45', '60', '27', '6d', '64', '41', 'c8', '9e', 'da', 'ff', '89', '39', '0c', '4d', '02', '32', '98', '25', '42', 'b1', 'e5', 'b4', '52', 'f0', 'ac', 'd9', 'e0', '75', '0a', '8b', '2c', '38', '7d', '12', 'dc', 'ef', '94', '55', 'a0', 'e1', 'e7', '04', 'be', 'f1', '7e', 'ea', '49', 'fc', 'e3', 'e4', 'b9', '6e', '31', 'f9', '62', 'b6', '53', '1a', 'd2', '70', '34', '18', '4b', 'db', '79', '85', 'c2', '88', '65', '1e', '67', 'ec', 'ab', 'a3', 'a7', '78', '0e', 'd6', '28', 'f4', '66', 'cb', 'a5', 'images.csv', 'a8', 'bc', 'e6', '56', '72', '6c', '16', '7b', 'bb', 'af', '9f', 'bd', 'ba', '13', '99', '1d', '4e', '3e', '7f', '26', 'b5', 'fb', 'd4', '4c', '74', 'b0', 'c6', '00', 'c4', '08', '15', 'ae', '90', '69', 'balanced_dataset.csv', 'f8', '77', 'd1', 'f5', '3f', '9b', 'cc', '86', '95', 'f3', '43', 'ce', 'eb', '91', '3c', '71', 'ca', 'ed', '09', '58', '59', '5e', 'fe', '4f', '7c', 'c3', '2d', '2b', 'a4', '97', '30', '7a', '14', '6f', 'f6', '5a', '03', '76', '3a', 'a6', '8e', 'a9', '8d', '9d', '84', 'df', '3d', '1f', 'b7', '83', 'f2', '82', '57', 'e8', 'd7', 'complete_dataset.csv', 'dd', 'aa', '5f', '1c', '96', '46', '6b', 'a1', '21', '44', '40', '80', '9a', '11', 'de', '68', '4a', 'vqa.csv', '63', 'ad', 'd3', '37', '51', 'f7', 'e9', '01', '8c', '9c', 'fa', '33', 'c1', 'b3', '54', '48', '0b', '5c', 'ee', '8a', 'c9', 'c0', '29', '3b', 'fd', '5d', '2f', '24', '6a', '73', '93', '2a']\nLoading CLIP model and processor...\nLoading BART model for BARTScore...\nExtracted 558 unique single-word answers from dataset\nOriginal dataset had 1809 distinct answers\nAfter frequency filtering (5+ occurrences): 558 answers\nAfter ensuring single-word only: 558 answers\n\nTotal dataset size: 45482 samples\nFull dataset: 45482 samples\n50% dataset: 22741 samples\n25% dataset: 11370 samples\n\nEvaluating 25% Dataset dataset (11370 samples)...\n","output_type":"stream"},{"name":"stderr","text":"Processing 25% Dataset dataset: 100%|██████████| 11370/11370 [4:18:09<00:00,  1.36s/it] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3ca0bbe95db498b8fa1b4fde53b706c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae13e64e69fa4c8594100da086083f26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24b4e7faf4674615bbba932a86a61e70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7708883674f40dba21817258c73acf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.04G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6604307ccd6417b82192a98c42b09ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.04G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d891de0225c743bf88d9fcd6144794bf"}},"metadata":{}},{"name":"stdout","text":"\n25% Dataset Evaluation Results:\nNumber of samples evaluated: 11360\nExact Match Accuracy: 4.79%\nClose Match Accuracy (with typo tolerance): 5.33%\nF1 Score: 4.79%\nBLEU Score: 0.86%\nBERTScore F1: 51.15%\nROUGE-L Score: 5.21%\n\nTop misclassifications:\nPredicted 'phone' instead of 'yes': 193 times\nPredicted 'samsung' instead of 'yes': 157 times\nPredicted 'phone' instead of 'black': 123 times\nPredicted 'phone' instead of 'pink': 93 times\nPredicted 'phone' instead of 'blue': 84 times\nPredicted 'phone' instead of 'white': 58 times\nPredicted 'motorola' instead of 'yes': 56 times\nPredicted 'amazonbasics' instead of 'yes': 54 times\nPredicted 'phone' instead of 'yellow': 53 times\nPredicted 'samsung' instead of 'blue': 53 times\n\nEvaluating 50% Dataset dataset (22741 samples)...\n","output_type":"stream"},{"name":"stderr","text":"Processing 50% Dataset dataset:  17%|█▋        | 3963/22741 [1:30:33<7:09:03,  1.37s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3448224560.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;31m# Evaluate half dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'50% Dataset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"50% Dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;31m# Evaluate full dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/3448224560.py\u001b[0m in \u001b[0;36mevaluate_dataset\u001b[0;34m(dataset_sample, name)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;31m# Use the optimized prediction function with unique answers from dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mpredicted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0mground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/3448224560.py\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(example, unique_answers, batch_size)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;31m# Encode text prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_features\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/open_clip/model.py\u001b[0m in \u001b[0;36mencode_text\u001b[0;34m(self, text, normalize)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch_size, n_ctx, transformer.width]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_global_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_pool_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_projection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_projection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}