{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11742394,"sourceType":"datasetVersion","datasetId":7371288},{"sourceId":11754073,"sourceType":"datasetVersion","datasetId":7379072},{"sourceId":11756195,"sourceType":"datasetVersion","datasetId":7380327},{"sourceId":11758521,"sourceType":"datasetVersion","datasetId":7381656}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BLIP-2 Baseline Evaluation","metadata":{}},{"cell_type":"code","source":"!pip install bert_score\n!pip install rouge_score\n\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\n\nimport os\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom bert_score import score\n\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.translate.meteor_score import meteor_score\n# =====================\n# CONFIGURATION\n# =====================\nCSV_PATH = \"/kaggle/input/complete-vqa/complete_vqa.csv\"\nIMAGE_ROOT_DIR = \"/kaggle/input/images/images/images/small\"\n\n# =====================\n# LOAD VQA CSV\n# =====================\nprint(\"Loading VQA dataset...\")\ndf = pd.read_csv(CSV_PATH)\ndf = df.sample(frac=0.25, random_state=42)\ndf = df.head(100) # for checking if code is fine\n\n# =====================\n# HELPER FUNCTIONS\n# =====================\ndef get_full_image_path(rel_path):\n    full_path = os.path.join(IMAGE_ROOT_DIR, rel_path)\n    return full_path if os.path.exists(full_path) else None\n\ndef load_image(image_path):\n    return Image.open(image_path).convert(\"RGB\")\n\n# =====================\n# LOAD BLIP-2 MODEL\n# =====================\nprint(\"Loading BLIP-2 model...\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-opt-2.7b\",\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n)\nmodel.to(device)\n\n# =====================\n# RUN INFERENCE\n# =====================\nprint(\"Running inference...\")\npredictions = []\n\nfor idx, row in tqdm(df.iterrows(), total=len(df)):\n    image_path = get_full_image_path(row[\"image_path\"])\n\n    if image_path is None:\n        predictions.append(\"Error: Image not found\")\n        continue\n\n    question = row[\"question\"]\n    prompt = f\"Question: {question} Answer in one word:\" \n\n    raw_image = load_image(image_path)\n    inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(\n        device, torch.float16 if device == \"cuda\" else torch.float32\n    )\n    generated_ids = model.generate(**inputs, max_new_tokens=10)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    answer = generated_text.strip()\n    \n    answer = answer.replace(prompt, \"\").strip()\n    \n    predictions.append(answer)\n\n# =====================\n# EVALUATION\n# =====================\nprint(\"\\nEvaluating predictions...\")\n\ndf[\"prediction\"] = predictions\n\n# 1. Exact Match Accuracy\nprint(\"Calculating Exact Match Accuracy...\")\ndf[\"correct\"] = df[\"answer\"].str.lower().str.strip() == df[\"prediction\"].str.lower().str.strip()\naccuracy = df[\"correct\"].mean()\n\n# 2. Filter valid entries\nvalid_df = df[\n    ~df[\"prediction\"].astype(str).str.startswith(\"Error\") &\n    df[\"answer\"].apply(lambda x: isinstance(x, str)) &\n    df[\"prediction\"].apply(lambda x: isinstance(x, str))\n]\n\nreferences = valid_df[\"answer\"].tolist()\ncandidates = valid_df[\"prediction\"].tolist()\n\n# 3. BERTScore\nprint(\"Calculating BERTScore...\")\nP, R, F1 = score(candidates, references, lang=\"en\", verbose=True)\nvalid_df[\"bertscore_f1\"] = F1.cpu()\navg_bertscore = F1.mean().item()\n\n# 4. ROUGE-L F1\nprint(\"Calculating ROUGE-L F1...\")\nscorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\nrouge_l_f1s = [\n    scorer.score(ref, cand)[\"rougeL\"].fmeasure\n    for ref, cand in zip(references, candidates)\n]\nvalid_df[\"rougeL_f1\"] = rouge_l_f1s\navg_rouge_l_f1 = sum(rouge_l_f1s) / len(rouge_l_f1s)\n\n# 6. METEOR Score\nprint(\"Calculating METEOR...\")\nmeteor_scores = [\n    meteor_score([ref.split()], cand.split())\n    for ref, cand in zip(references, candidates)\n]\nvalid_df[\"meteor\"] = meteor_scores\navg_meteor = sum(meteor_scores) / len(meteor_scores)\n\n# Assign back to full df\nfor col in [\"bertscore_f1\", \"rougeL_f1\", \"meteor\"]:\n    df.loc[valid_df.index, col] = valid_df[col].values\n\nblank_row = {col: \"\" for col in df.columns}\ndf = pd.concat([df, pd.DataFrame([blank_row])], ignore_index=True)\n\n# Append summary row\nsummary_row = {\n    \"image_path\": \"AVERAGES\",\n    \"question\": \"\",\n    \"answer\": \"\",\n    \"prediction\": \"\",\n    \"correct\": accuracy,\n    \"bertscore_f1\": avg_bertscore,\n    \"rougeL_f1\": avg_rouge_l_f1,\n    \"meteor\": avg_meteor\n}\ndf = pd.concat([df, pd.DataFrame([summary_row])], ignore_index=True)\n\n# Print average scores\nprint(\"\\nAverage Scores:\")\nprint(f\"  Exact Match Accuracy: {accuracy:.4f}\")\nprint(f\"  BERTScore F1:         {avg_bertscore:.4f}\")\nprint(f\"  ROUGE-L F1:           {avg_rouge_l_f1:.4f}\")\nprint(f\"  METEOR:               {avg_meteor:.4f}\")\n\n# Save results\ndf.to_csv(\"vqa_blip2_baseline_results.csv\", index=False)\nprint(\"Results saved to vqa_blip2_baseline_results.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:19:05.544428Z","iopub.execute_input":"2025-05-10T12:19:05.545038Z","iopub.status.idle":"2025-05-10T12:20:01.779650Z","shell.execute_reply.started":"2025-05-10T12:19:05.545010Z","shell.execute_reply":"2025-05-10T12:20:01.778969Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: bert_score in /usr/local/lib/python3.11/dist-packages (0.3.13)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\nLoading VQA dataset...\nLoading BLIP-2 model...\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d080057efc2b4f8ab67c8b35281007d1"}},"metadata":{}},{"name":"stdout","text":"Running inference...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:13<00:00,  7.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluating predictions...\nCalculating Exact Match Accuracy...\nCalculating BERTScore...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53c332392d764e84bdea20b601d886d7"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ad63bb051fd4fc9958856ae38702e61"}},"metadata":{}},{"name":"stdout","text":"done in 0.15 seconds, 685.70 sentences/sec\nCalculating ROUGE-L F1...\nCalculating METEOR...\n\nAverage Scores:\n  Exact Match Accuracy: 0.4800\n  BERTScore F1:         0.8175\n  ROUGE-L F1:           0.4867\n  METEOR:               0.2695\nResults saved to vqa_blip2_baseline_results.csv\n","output_type":"stream"},{"name":"stderr","text":"Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"# Calculating more scores for analysis","metadata":{}},{"cell_type":"code","source":"# computing some more scores like BART and BERT variants using the output of the above cell\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import f1_score\nfrom bert_score import score \nfrom rouge_score import rouge_scorer\nimport torch\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\n# Load BART model and tokenizer\nbart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nbart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\nbart_model.eval()\n\n# Use GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbart_model.to(device)\n\n# Load your CSV\ndf = pd.read_csv(\"/kaggle/input/baseline-blip2/blip2_baseline_results_50.csv\")  # replace accordingly with output of previous cell\ndf = df.drop(columns=[\"rougeL_f1\", \"bertscore_f1\"], errors=\"ignore\")\ndf = df[df['image_path'] != 'AVERAGES']\n\n# Fill missing predictions with empty strings for uniformity\ndf['prediction'] = df['prediction'].fillna(\"\").astype(str)\n\n# Initialize metric lists\nf1s = []\naccuracies = []\nbertscore_precisions = []\nbertscore_recalls = []\nbertscore_f1s = []\nbartscores = []\nrouge_l_f1s = []\n\n# Compute BERTScore only for valid predictions\nvalid_mask = df['prediction'].str.strip() != \"\"\nvalid_preds = df.loc[valid_mask, 'prediction'].tolist()\nvalid_refs = df.loc[valid_mask, 'answer'].astype(str).tolist()\n\nif valid_preds:\n    print(\"Calculating BERTScore...\")\n    P, R, F1 = score(valid_preds, valid_refs, lang=\"en\", verbose=True)\nelse:\n    P, R, F1 = [], [], []\n\n# ROUGE scorer\nrouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n\n# Row-wise processing\nbert_idx = 0  # index to track bert_score results\nfor idx, row in df.iterrows():\n    ref = str(row['answer']).strip()\n    pred = str(row['prediction']).strip()\n    correct = row.get('correct', False)\n\n    if pred == \"\":\n        # Empty prediction — assign zeros\n        f1s.append(0.0)\n        accuracies.append(1 if correct is True or correct == \"True\" else 0)\n        bertscore_precisions.append(0.0)\n        bertscore_recalls.append(0.0)\n        bertscore_f1s.append(0.0)\n        bartscores.append(0.0)\n        rouge_l_f1s.append(0.0)\n        continue\n\n    # F1 (token-based)\n    ref_tokens = ref.lower().split()\n    pred_tokens = pred.lower().split()\n    common = set(ref_tokens) & set(pred_tokens)\n    precision = len(common) / len(pred_tokens) if pred_tokens else 0\n    recall = len(common) / len(ref_tokens) if ref_tokens else 0\n    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n    f1s.append(f1)\n\n    # Accuracy\n    accuracy = 1 if correct is True or correct == \"True\" else 0\n    accuracies.append(accuracy)\n\n    # BERTScore\n    bertscore_precisions.append(P[bert_idx].item())\n    bertscore_recalls.append(R[bert_idx].item())\n    bertscore_f1s.append(F1[bert_idx].item())\n    bert_idx += 1\n\n    # BARTScore (negative log-likelihood)\n    inputs = bart_tokenizer(pred, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n    with bart_tokenizer.as_target_tokenizer():\n        labels = bart_tokenizer(ref, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(device)\n\n    with torch.no_grad():\n        outputs = bart_model(**inputs, labels=labels)\n        log_likelihood = -outputs.loss.item()\n    bartscores.append(log_likelihood)\n\n    # ROUGE-L F1\n    rouge_l_f1 = rouge.score(ref, pred)['rougeL'].fmeasure\n    rouge_l_f1s.append(rouge_l_f1)\n\n# Add new columns to DataFrame\ndf['f1'] = f1s\ndf['accuracy'] = accuracies\ndf['bertscore_precision'] = bertscore_precisions\ndf['bertscore_recall'] = bertscore_recalls\ndf['bertscore_f1'] = bertscore_f1s\ndf['bartscore'] = bartscores\ndf['rouge_l_f1'] = rouge_l_f1s\n\n# Compute average row\navg_row = {\n    'image_path': 'AVERAGE',\n    'question': '',\n    'answer': '',\n    'prediction': '',\n    'correct': '',\n    'meteor': df['meteor'].mean() if 'meteor' in df else '',\n    'f1': np.mean(f1s),\n    'accuracy': np.mean(accuracies),\n    'bertscore_precision': np.mean(bertscore_precisions),\n    'bertscore_recall': np.mean(bertscore_recalls),\n    'bertscore_f1': np.mean(bertscore_f1s),\n    'bartscore': np.mean(bartscores),\n    'rouge_l_f1': np.mean(rouge_l_f1s),\n}\n\n# Create a blank row (empty values)\nblank_row = pd.DataFrame([{\n    'image_path': '',\n    'question': '',\n    'answer': '',\n    'prediction': '',\n    'correct': '',\n    'meteor': '',\n    'f1': '',\n    'accuracy': '',\n    'bertscore_precision': '',\n    'bertscore_recall': '',\n    'bertscore_f1': '',\n    'bartscore': '',\n    'rouge_l_f1': '',\n}])\n\n# Neatly print average results\nprint(\"\\n=== Average Scores for BLIP2 using 50% of dataset ===\")\nprint(f\"F1 Score:              {avg_row['f1']:.4f}\")\nprint(f\"Accuracy:              {avg_row['accuracy']:.4f}\")\nprint(f\"BERTScore Precision:   {avg_row['bertscore_precision']:.4f}\")\nprint(f\"BERTScore Recall:      {avg_row['bertscore_recall']:.4f}\")\nprint(f\"BERTScore F1:          {avg_row['bertscore_f1']:.4f}\")\nprint(f\"BARTScore:             {avg_row['bartscore']:.4f}\")\nprint(f\"ROUGE-L F1:            {avg_row['rouge_l_f1']:.4f}\")\nif 'meteor' in avg_row and avg_row['meteor'] != '':\n    print(f\"METEOR:                {avg_row['meteor']:.4f}\")\n\n# Append and save\ndf_with_avg = pd.concat([df, blank_row, pd.DataFrame([avg_row])], ignore_index=True)\ndf_with_avg.to_csv(\"scored_output.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:51:26.671068Z","iopub.execute_input":"2025-05-10T13:51:26.671847Z","iopub.status.idle":"2025-05-10T13:57:45.039239Z","shell.execute_reply.started":"2025-05-10T13:51:26.671821Z","shell.execute_reply":"2025-05-10T13:57:45.038578Z"}},"outputs":[{"name":"stdout","text":"Calculating BERTScore...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/34 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fbdef51fdaf4fd58710b3a34ad1dcb3"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/290 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e10c3f77bf41400fb08d2a68e80aec23"}},"metadata":{}},{"name":"stdout","text":"done in 5.24 seconds, 3541.32 sentences/sec\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n=== Average Scores for BLIP2 using 50% of dataset ===\nF1 Score:              0.4109\nAccuracy:              0.4043\nBERTScore Precision:   0.7904\nBERTScore Recall:      0.7869\nBERTScore F1:          0.7882\nBARTScore:             -3.8912\nROUGE-L F1:            0.4258\nMETEOR:                0.2261\n","output_type":"stream"}],"execution_count":59}]}